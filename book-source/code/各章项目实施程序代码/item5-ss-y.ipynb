{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e29cae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正常邮件的文件列表 ['normal-mail1.txt', 'normal-mail2.txt', 'normal-mail3.txt', 'normal-mail4.txt', 'normal-mail5.txt', 'normal-mail6.txt', 'normal-mail7.txt', 'normal-mail8.txt', 'normal-mail9.txt']\n",
      "垃圾邮件的文件列表 ['spam-mail1.txt', 'spam-mail2.txt', 'spam-mail3.txt', 'spam-mail4.txt', 'spam-mail5.txt', 'spam-mail6.txt', 'spam-mail7.txt', 'spam-mail8.txt', 'spam-mail9.txt']\n",
      "停用词文件内容： ['啊', '阿', '哎', '哎呀', '唉', '于是', '还']\n"
     ]
    }
   ],
   "source": [
    "#项目5-项目实施代码\n",
    "#导入os模块\n",
    "import os\n",
    "\n",
    "#获取正常邮件和垃圾邮件的文件列表\n",
    "normalFileList=os.listdir(\"../item5/item5-ss-data/normal/\")\n",
    "spamFileList=os.listdir(\"../item5/item5-ss-data/spam/\")\n",
    "print(\"正常邮件的文件列表\",normalFileList)\n",
    "print(\"垃圾邮件的文件列表\",spamFileList)\n",
    "\n",
    "#获取停用词表，用于对停用词进行过滤\n",
    "stopList=[]   #存放停用词\n",
    "for line in open(\"../item5/item5-ss-data/stopwords.txt\",encoding='utf-8'):\n",
    "    stopList.append(line[:len(line)-1])\n",
    "print(\"停用词文件内容：\",stopList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aba1f732",
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入需要的库\n",
    "from jieba import cut#导入中文分词库（结巴分词）\n",
    "from re import sub\n",
    "\n",
    "#定义getWords()函数，用于提取指定文件（邮件文件）的词语\n",
    "def getWords(file,stopList):\n",
    "    wordsList=[]\n",
    "    for line in open(file,encoding='utf-8'):\n",
    "        line=line.strip() #移除字符串头尾指定的字符（默认为空格或换行符）或字符序列\n",
    "        #过滤干扰字符或无效字符\n",
    "        line=sub(r'[.【】0-9、——，。！\\~*]','',line)  \n",
    "        line=cut(line)\n",
    "        #过滤长度为1的单个字\n",
    "        line=filter(lambda word:len(word)>1,line) \n",
    "        wordsList.extend(line)\n",
    "        #过滤停用词，剩余有效词语\n",
    "        words=[]\n",
    "        for i in wordsList:\n",
    "            if i not in stopList and i.strip()!='' and i!=None:\n",
    "                words.append(i)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e9ae657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集中所有的有效词语列表：\n",
      "[['期刊', '主要', '栏目', '技术', '应用', '投稿', '邮箱', 'xx', 'com'], ['期刊', '主要', '栏目', '数据', '投稿', '邮箱', 'xx', 'com'], ['某某', '期刊', '主要', '栏目', '数据', '投稿', '邮箱', 'xx', 'com'], ['期刊', '主要', '栏目', '计算', '投稿', '邮箱', 'xx', 'com'], ['期刊', '主要', '栏目', '人工智能', '投稿', '邮箱', 'xx', 'com'], ['期刊', '主要', '栏目', '网络设备', '投稿', '邮箱', 'xx', 'com'], ['期刊', '主要', '栏目', '计算机', '基础', '投稿', '邮箱', 'xx', 'com'], ['期刊', '主要', '栏目', '网络', '制图', '投稿', '邮箱', 'xx', 'com'], ['期刊', '主要', '栏目', '网站', '绘图', '投稿', '邮箱', 'xx', 'com'], ['张老师', '您好', '上次', '推荐', '资料', '来说', '帮助', '很大', '希望', '推荐', '一些', '资料', '非常感谢'], ['小李', '你好', '论文', '需要', '修改', '具体', '修改意见', '附件', '查收'], ['李老师', '您好', '论文', '已经', '修改', '修改', '完成', '内容', '附件', '查收'], ['小李', '你好', '论文', '需要', '修改', '具体', '修改意见', '附件', '查收'], ['小张', '你好', '论文', '需要', '修改', '具体', '修改意见', '附件', '查收'], ['张老师', '您好', '论文', '修改', '具体内容', '附件', '查收'], ['李老师', '你好', '论文', '修改', '中等', '修改', '完成', '沟通'], ['小张', '你好', '论文', '需要', '修改', '具体', '修改意见', '附件', '查收'], ['小张', '你好', '论文', '需要', '修改', '具体', '修改意见', '附件', '查收']]\n",
      "训练集中出现频次最高的前10个词语:\n",
      "['修改', '期刊', '主要', '栏目', '投稿', '邮箱', 'xx', 'com', '论文', '附件']\n"
     ]
    }
   ],
   "source": [
    "#提取训练集所有文件中的词语与出现频次最高的前10个词语\n",
    "\n",
    "#导入需要的库\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "#提取训练集所有文件中的词语\n",
    "allwords=[]\n",
    "for spamfile in spamFileList:\n",
    "    words=getWords(\"../item5/item5-ss-data/spam/\"+spamfile,stopList)\n",
    "    allwords.append(words)\n",
    "for normalfile in normalFileList:\n",
    "    words=getWords(\"../item5/item5-ss-data/normal/\"+normalfile,stopList)\n",
    "    allwords.append(words)\n",
    "print(\"训练集中所有的有效词语列表：\")\n",
    "print(allwords)\n",
    "\n",
    "#提取训练集中出现频次最高的前10个词语\n",
    "frep=Counter(chain(*allwords))  #获取有效词语出现的频次\n",
    "topTen=frep.most_common(10)#获取出现频次最高的前10个词语和对应的频次\n",
    "topWords=[w[0] for w in topTen] #获取出现频次最高的前10个词语\n",
    "print(\"训练集中出现频次最高的前10个词语:\")\n",
    "print(topWords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6446d44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10个高频词语在每封邮件中出现的次数：\n",
      "[[0 1 1 1 1 1 1 1 0 0]\n",
      " [0 1 1 1 1 1 1 1 0 0]\n",
      " [0 1 1 1 1 1 1 1 0 0]\n",
      " [0 1 1 1 1 1 1 1 0 0]\n",
      " [0 1 1 1 1 1 1 1 0 0]\n",
      " [0 1 1 1 1 1 1 1 0 0]\n",
      " [0 1 1 1 1 1 1 1 0 0]\n",
      " [0 1 1 1 1 1 1 1 0 0]\n",
      " [0 1 1 1 1 1 1 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 1 1]\n",
      " [2 0 0 0 0 0 0 0 1 1]\n",
      " [1 0 0 0 0 0 0 0 1 1]\n",
      " [1 0 0 0 0 0 0 0 1 1]\n",
      " [1 0 0 0 0 0 0 0 1 1]\n",
      " [2 0 0 0 0 0 0 0 1 0]\n",
      " [1 0 0 0 0 0 0 0 1 1]\n",
      " [1 0 0 0 0 0 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "#获取10个高频词语在每封邮件中出现的次数\n",
    "#导入需要的库\n",
    "import numpy as np\n",
    "\n",
    "vector=[]\n",
    "for words in allwords:\n",
    "    temp=list(map(lambda x:words.count(x),topWords)) #每个高频词语在每封邮件中出现的次数\n",
    "    vector.append(temp)\n",
    "vector=np.array(vector)\n",
    "print(\"10个高频词语在每封邮件中出现的次数：\")\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73efae2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#使用朴素贝叶斯算法训练模型\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#为数据集打标签，1表示垃圾邮件，0表示正常邮件\n",
    "target=np.array([1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0])\n",
    "x,y=vector,target\n",
    "\n",
    "#建立多项式朴素贝叶斯模型并进行训练\n",
    "model=MultinomialNB()\n",
    "model.fit(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56328e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"normal-test.txt\"是正常邮件\n",
      "\"spam-test.txt\"是垃圾邮件\n"
     ]
    }
   ],
   "source": [
    "#使用模型预测新的未知邮件的类别\n",
    "\n",
    "#获取测试邮件文件列表\n",
    "test=os.listdir(\"../item5/item5-ss-data/test\")\n",
    "#使用模型进行预测\n",
    "for testFile in test:\n",
    "    words=getWords(\"../item5/item5-ss-data/test/\"+testFile,stopList)#调用getWords()函数，提取文件词语\n",
    "    test_x=np.array(tuple(map(lambda x:words.count(x),topWords))) #提取10个高频词语分别在邮件中出现的次数\n",
    "    result=model.predict(test_x.reshape(1,-1))\n",
    "    if result==1:\n",
    "        print('\"'+testFile+'\"'+\"是垃圾邮件\")\n",
    "    else:\n",
    "         print('\"'+testFile+'\"'+\"是正常邮件\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcec7279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d20d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
